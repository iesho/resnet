Custom implementation of residual neural networks, following the seminal work by Kaiming He et al, Deep Residual Learning for Image Recognition. Demonstrated that a very deep, "plain" convolutional neural network without skip connections performs worse as the number of layers increases. This limitation is overcome by including skip connections (ResNet). For example, a 56-layer convolutional neural network performs better than a 20-layer one when skip connections are included. Models were trained on CIFAR10 dataset. 